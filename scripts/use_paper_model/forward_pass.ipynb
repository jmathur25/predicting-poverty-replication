{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["This file uses a Python2 library from Github (https://github.com/vadimkantorov/caffemodel2pytorch) to convert Caffe models into PyTorch. The original model was trained with Caffe. Remarkably, you don't ever have to install caffe for it to work!\n", "\n", "I converted it to work on Python3.\n", "\n", "\n", "Run `merge_nps` to combine the files created in this script. I break the computation into pieces because sometimes the script fails partially through."]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["from caffemodel2pytorch import *"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["caffemodel2pytorch: loading model from [predicting_poverty_trained.caffemodel] in HDF5 format failed [Unable to open file (file signature not found)], falling back to caffemodel format\n", "caffemodel2pytorch: loaded model from [predicting_poverty_trained.caffemodel] in caffemodel format\n"]}], "source": ["model = Net(\n", "    prototxt = 'predicting_poverty_deploy.prototxt',\n", "    weights = 'predicting_poverty_trained.caffemodel',\n", "    caffe_proto = 'https://raw.githubusercontent.com/BVLC/caffe/master/src/caffe/proto/caffe.proto'\n", ")"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/plain": ["Net(\n", "  (conv1): Convolution(3, 64, kernel_size=(11, 11), stride=(4, 4))\n", "  (relu1): ReLU()\n", "  (norm1): LocalResponseNorm(5, alpha=0.0005000000237487257, beta=0.75, k=1.0)\n", "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "  (conv2): Convolution(64, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n", "  (relu2): ReLU()\n", "  (norm2): LocalResponseNorm(5, alpha=0.0005000000237487257, beta=0.75, k=1.0)\n", "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "  (conv3): Convolution(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "  (relu3): ReLU()\n", "  (conv4): Convolution(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "  (relu4): ReLU()\n", "  (conv5): Convolution(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "  (relu5): ReLU()\n", "  (pool5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "  (conv6): Convolution(256, 4096, kernel_size=(6, 6), stride=(6, 6))\n", "  (relu6): ReLU()\n", "  (conv7): Convolution(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n", "  (relu7): ReLU()\n", "  (conv8): Convolution(4096, 3, kernel_size=(1, 1), stride=(1, 1))\n", "  (pool6): AvgPool2d(kernel_size=2, stride=1, padding=0)\n", "  (prob): Softmax()\n", ")"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["model"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# we strip the last layers, so the output is just what was at the conv7 layer\n", "# the paper uses the data at this layer as the \"features\" for the image\n", "del model.prob, model.pool6, model.conv8, model.relu7"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": ["Net(\n", "  (conv1): Convolution(3, 64, kernel_size=(11, 11), stride=(4, 4))\n", "  (relu1): ReLU()\n", "  (norm1): LocalResponseNorm(5, alpha=0.0005000000237487257, beta=0.75, k=1.0)\n", "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "  (conv2): Convolution(64, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n", "  (relu2): ReLU()\n", "  (norm2): LocalResponseNorm(5, alpha=0.0005000000237487257, beta=0.75, k=1.0)\n", "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "  (conv3): Convolution(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "  (relu3): ReLU()\n", "  (conv4): Convolution(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "  (relu4): ReLU()\n", "  (conv5): Convolution(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "  (relu5): ReLU()\n", "  (pool5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "  (conv6): Convolution(256, 4096, kernel_size=(6, 6), stride=(6, 6))\n", "  (relu6): ReLU()\n", "  (conv7): Convolution(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n", ")"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "model.eval()\n", "model.to(device)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["from torchvision import datasets, models, transforms"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# turn the data into a PyTorch Tensor\n", "data_transforms = {\n", "    'transform': transforms.Compose([\n", "        transforms.ToTensor(),\n", "    ]),\n", "}"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "from PIL import Image\n", "import numpy as np\n", "\n", "# takes a \n", "def filename_to_im_tensor(file):\n", "    im = plt.imread(file)[:,:,:3]\n", "    im = (im*256)\n", "    im -= np.array([103.334, 107.8797, 107.4072])\n", "    # convert to BGR, their Github code says to do this\n", "    input_img = im[:, :, [2, 1, 0]]\n", "    # convert to D,H,W\n", "    input_img = np.transpose(input_img, [2, 0, 1])\n", "    # subtract the mean to normalize the image, their Github code says to do this\n", "    mean_bgr = [103.334, 107.8797, 107.4072]\n", "    for i in range(0, 3):\n", "        input_img[i, :, :] = input_img[i, :, :] - mean_bgr[i]\n", "    \n", "    im = Image.fromarray(im.astype(np.uint8))\n", "    im = data_transforms['transform'](im)\n", "    return im[None].to(device)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["import os\n", "ims = os.listdir('../process_data/data/ims_malawi_2016/')"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": ["23464"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["len(ims)"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["feats = np.zeros((len(ims), 4096))"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, 10100, 10200, 10300, 10400, 10500, 10600, 10700, 10800, 10900, 11000, 11100, 11200, 11300, 11400, 11500, 11600, 11700, 11800, 11900, 12000, 12100, 12200, 12300, 12400, 12500, 12600, 12700, 12800, 12900, 13000, 13100, 13200, 13300, 13400, 13500, 13600, 13700, 13800, 13900, 14000, 14100, 14200, 14300, 14400, 14500, 14600, 14700, 14800, 14900, 15000, 15100, 15200, 15300, 15400, 15500, 15600, 15700, 15800, 15900, 16000, 16100, 16200, 16300, 16400, 16500, 16600, 16700, 16800, 16900, 17000, 17100, 17200, 17300, 17400, 17500, 17600, 17700, 17800, 17900, 18000, 18100, 18200, 18300, 18400, 18500, 18600, 18700, 18800, 18900, 19000, 19100, 19200, 19300, 19400, 19500, 19600, 19700, 19800, 19900, 20000, 20100, 20200, 20300, 20400, 20500, 20600, 20700, 20800, 20900, 21000, 21100, 21200, 21300, 21400, 21500, 21600, 21700, 21800, 21900, 22000, 22100, 22200, 22300, 22400, 22500, 22600, 22700, 22800, 22900, 23000, 23100, 23200, 23300, 23400, "]}], "source": ["i = 0\n", "batch_size = 4\n", "pre = '../process_data/data/ims_malawi_2016/{}'\n", "\n", "# this approach uses batching and should offer a speed-up over passing one image at a time by nearly 10x\n", "# runtime should be 5-7 minutes vs 45+ for a full forward pass\n", "while i + batch_size < len(ims):\n", "    ims_as_tensors = torch.cat([filename_to_im_tensor(pre.format(ims[i+j])) for j in range(batch_size)], 0)\n", "    feats[i:i+batch_size,:] = np.squeeze(model(ims_as_tensors)['conv7'].cpu().detach().numpy())\n", "    i += batch_size\n", "    if i % 100 == 0:\n", "        print(i, end=', ')\n", "\n", "# does the final batch of remaining images\n", "if len(ims) - i != 0:\n", "    rem = len(ims) - i\n", "    ims_as_tensors = torch.cat([filename_to_im_tensor(pre.format(ims[i+j])) for j in range(rem)], 0)\n", "    feats[i:i+rem,:] = np.squeeze(model(ims_as_tensors)['conv7'].cpu().detach().numpy())\n", "    i += rem"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["np.save('forward_feats.npy', feats)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 2}