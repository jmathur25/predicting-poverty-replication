{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the images marked as valid per cluster, we pass them through the CNN and extract their feature vectors. the results are stored at a per-country basis. For example, all Malawi feature extractions will go into results/malawi_2016/cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '..'\n",
    "COUNTRIES_DIR = os.path.join(BASE_DIR, 'data', 'countries')\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, 'results')\n",
    "CNN_TRAIN_IMAGE_DIR = os.path.join(BASE_DIR, 'data', 'cnn_images')\n",
    "CNN_DIR = os.path.join(BASE_DIR, 'models', 'trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "for country in ['malawi_2016', 'ethiopia_2015', 'nigeria_2015']:\n",
    "    os.makedirs(os.path.join(RESULTS_DIR, country), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extract with CNN\n",
    "If you have run this step before, you can skip it and run the commented out code in the next section to quick-start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = pd.read_csv(os.path.join(PROCESSED_DIR, 'image_download_actual.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_lat</th>\n",
       "      <th>image_lon</th>\n",
       "      <th>cluster_lat</th>\n",
       "      <th>cluster_lon</th>\n",
       "      <th>cons_pc</th>\n",
       "      <th>nightlights</th>\n",
       "      <th>country</th>\n",
       "      <th>nightlights_bin</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.09515_35.17229723579403_-17.09515_35.21721...</td>\n",
       "      <td>-17.095150</td>\n",
       "      <td>35.172297</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.08017807859801_35.17229723579403_-17.09515...</td>\n",
       "      <td>-17.080178</td>\n",
       "      <td>35.172297</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.125093842803985_35.18726915719602_-17.0951...</td>\n",
       "      <td>-17.125094</td>\n",
       "      <td>35.187269</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.140065764205975_35.20224107859801_-17.0951...</td>\n",
       "      <td>-17.140066</td>\n",
       "      <td>35.202241</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.065206157196016_35.20224107859801_-17.0951...</td>\n",
       "      <td>-17.065206</td>\n",
       "      <td>35.202241</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name  image_lat  image_lon  \\\n",
       "0  -17.09515_35.17229723579403_-17.09515_35.21721... -17.095150  35.172297   \n",
       "1  -17.08017807859801_35.17229723579403_-17.09515... -17.080178  35.172297   \n",
       "2  -17.125093842803985_35.18726915719602_-17.0951... -17.125094  35.187269   \n",
       "3  -17.140065764205975_35.20224107859801_-17.0951... -17.140066  35.202241   \n",
       "4  -17.065206157196016_35.20224107859801_-17.0951... -17.065206  35.202241   \n",
       "\n",
       "   cluster_lat  cluster_lon   cons_pc  nightlights country  nightlights_bin  \\\n",
       "0    -17.09515    35.217213  1.423239     0.025206      mw                0   \n",
       "1    -17.09515    35.217213  1.423239     0.025206      mw                0   \n",
       "2    -17.09515    35.217213  1.423239     0.025206      mw                0   \n",
       "3    -17.09515    35.217213  1.423239     0.025206      mw                0   \n",
       "4    -17.09515    35.217213  1.423239     0.025206      mw                0   \n",
       "\n",
       "   is_train  \n",
       "0      True  \n",
       "1      True  \n",
       "2     False  \n",
       "3      True  \n",
       "4      True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda as backend\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device} as backend')\n",
    "model = torch.load(CNN_DIR, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rip off the final layers\n",
    "model.classifier = model.classifier[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c14651be94a48bc95f289f5310e41a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=495.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603833349e554916a46a4b00e5840582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=307.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4e14b2c64548029212b61ab40db408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=132.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# custom dataset for fast image loading and processing\n",
    "# does not follow the usual style of folder -> folder for each class -> image\n",
    "# we just want one folder with images\n",
    "class ForwardPassDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, transformer):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_list = os.listdir(self.image_dir)\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_list[index]\n",
    "\n",
    "        # Load image\n",
    "        X = self.filename_to_im_tensor(self.image_dir + '/' + image_name)\n",
    "        \n",
    "        # dataloaders need to return a label, but for the forward pass we don't really care\n",
    "        return X, -1\n",
    "    \n",
    "    def filename_to_im_tensor(self, file):\n",
    "        im = plt.imread(file)[:,:,:3]\n",
    "        im = self.transformer(im)\n",
    "        return im\n",
    "\n",
    "model.eval()  \n",
    "classes = [0, 1, 2]\n",
    "# shape of final array will be (num_validation_images, 4096)\n",
    "# we also want to record the image each index represents\n",
    "feats = np.zeros(((~df_images['is_train']).sum(), 4096))\n",
    "image_order = []\n",
    "i = 0\n",
    "for c in classes:\n",
    "    # use the validation images to do the forward pass\n",
    "    dataset = ForwardPassDataset(os.path.join(CNN_TRAIN_IMAGE_DIR, 'valid', str(c)), transformer)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "    image_order += dataset.image_list\n",
    "    # forward pass for this class\n",
    "    for inputs, _ in tqdm(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        feats[i:i+len(inputs),:] = outputs.cpu().detach().numpy()\n",
    "        i += len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.26415765, -0.28140783, -0.29993755, ...,  0.33739716,\n",
       "        -0.96456331, -0.95310527],\n",
       "       [ 0.55027246, -0.06091447,  0.11403629, ..., -0.08996978,\n",
       "        -0.62236136, -0.96085918],\n",
       "       [ 0.52193987, -0.29220241, -0.45371717, ...,  0.34175205,\n",
       "        -1.1439786 , -0.85960728],\n",
       "       ...,\n",
       "       [-0.50936353,  0.39209121, -0.29870456, ...,  0.0661362 ,\n",
       "         0.43009469, -0.34069228],\n",
       "       [ 0.24428365,  0.07818466, -0.89307284, ...,  0.29522306,\n",
       "        -0.72958505, -1.24356151],\n",
       "       [-0.30123377,  0.6785413 , -0.19940855, ...,  0.14395328,\n",
       "         0.52420121, -1.16047859]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>feat_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.513181862198008_39.768191057994024_10.52815...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-14.632534157196016_34.981995235794024_-14.662...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-14.526346764205977_35.593520078598004_-14.481...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.4290196173360155_7.26950147266_7.45896346014...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-10.405547698244352_34.14279535535209_-10.4038...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name  feat_index\n",
       "0  10.513181862198008_39.768191057994024_10.52815...           0\n",
       "1  -14.632534157196016_34.981995235794024_-14.662...           1\n",
       "2  -14.526346764205977_35.593520078598004_-14.481...           2\n",
       "3  7.4290196173360155_7.26950147266_7.45896346014...           3\n",
       "4  -10.405547698244352_34.14279535535209_-10.4038...           4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_pass_df = pd.DataFrame.from_dict({'image_name': image_order, 'feat_index': np.arange(len(image_order))})\n",
    "forward_pass_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumption = pd.merge(left=df_images, right=forward_pass_df, on='image_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have we maintained all validation images?\n",
    "assert len(df_consumption) == (~df_images['is_train']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_lat</th>\n",
       "      <th>image_lon</th>\n",
       "      <th>cluster_lat</th>\n",
       "      <th>cluster_lon</th>\n",
       "      <th>cons_pc</th>\n",
       "      <th>nightlights</th>\n",
       "      <th>country</th>\n",
       "      <th>nightlights_bin</th>\n",
       "      <th>is_train</th>\n",
       "      <th>feat_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.125093842803985_35.18726915719602_-17.0951...</td>\n",
       "      <td>-17.125094</td>\n",
       "      <td>35.187269</td>\n",
       "      <td>-17.095150</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.140065764205975_35.232184921401995_-17.095...</td>\n",
       "      <td>-17.140066</td>\n",
       "      <td>35.232185</td>\n",
       "      <td>-17.095150</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.065206157196016_35.262128764205976_-17.095...</td>\n",
       "      <td>-17.065206</td>\n",
       "      <td>35.262129</td>\n",
       "      <td>-17.095150</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.07737907859801_35.069727235794026_-17.0923...</td>\n",
       "      <td>-17.077379</td>\n",
       "      <td>35.069727</td>\n",
       "      <td>-17.092351</td>\n",
       "      <td>35.114643</td>\n",
       "      <td>1.266204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.137266764205975_35.08469915719602_-17.0923...</td>\n",
       "      <td>-17.137267</td>\n",
       "      <td>35.084699</td>\n",
       "      <td>-17.092351</td>\n",
       "      <td>35.114643</td>\n",
       "      <td>1.266204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name  image_lat  image_lon  \\\n",
       "0  -17.125093842803985_35.18726915719602_-17.0951... -17.125094  35.187269   \n",
       "1  -17.140065764205975_35.232184921401995_-17.095... -17.140066  35.232185   \n",
       "2  -17.065206157196016_35.262128764205976_-17.095... -17.065206  35.262129   \n",
       "3  -17.07737907859801_35.069727235794026_-17.0923... -17.077379  35.069727   \n",
       "4  -17.137266764205975_35.08469915719602_-17.0923... -17.137267  35.084699   \n",
       "\n",
       "   cluster_lat  cluster_lon   cons_pc  nightlights country  nightlights_bin  \\\n",
       "0   -17.095150    35.217213  1.423239     0.025206      mw                0   \n",
       "1   -17.095150    35.217213  1.423239     0.025206      mw                0   \n",
       "2   -17.095150    35.217213  1.423239     0.025206      mw                0   \n",
       "3   -17.092351    35.114643  1.266204     0.000000      mw                0   \n",
       "4   -17.092351    35.114643  1.266204     0.000000      mw                0   \n",
       "\n",
       "   is_train  feat_index  \n",
       "0     False         318  \n",
       "1     False        1861  \n",
       "2     False         836  \n",
       "3     False          18  \n",
       "4     False        1051  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_consumption.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Features\n",
    "For each country, we aggregate the image features per cluster and save them to results/country/cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_abbrv = ['mw', 'eth', 'ng']\n",
    "country_dir = ['malawi_2016', 'ethiopia_2015', 'nigeria_2015']\n",
    "\n",
    "for ca, cd in zip(country_abbrv, country_dir):\n",
    "    df_c = df_consumption[df_consumption['country'] == ca]\n",
    "    group = df_c.groupby(['cluster_lat', 'cluster_lon'])\n",
    "    x = np.zeros((len(group), 4096))\n",
    "    cluster_list = [] # the corresponding clusters (lat, lon) to the x aggregate feature array\n",
    "    for i, g in enumerate(group):\n",
    "        lat, lon = g[0]\n",
    "        im_sub = df_consumption[(df_consumption['cluster_lat'] == lat) & (df_consumption['cluster_lon'] == lon)].reset_index(drop=True)\n",
    "        agg_feats = np.zeros((len(im_sub), 4096))\n",
    "        for j, d in im_sub.iterrows():\n",
    "            agg_feats[j,:] = feats[d.feat_index]\n",
    "        agg_feats = agg_feats.mean(axis=0) # averages the features across all images in the cluster\n",
    "\n",
    "        x[i,:] = agg_feats\n",
    "        cluster_list.append([lat, lon])\n",
    "    # save to the correct directory\n",
    "    save_dir = os.path.join(RESULTS_DIR, cd, 'cnn')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.save(os.path.join(save_dir, 'cluster_feats.npy'), x)\n",
    "    pickle.dump(cluster_list, open(os.path.join(save_dir, 'cluster_order.pkl'), 'wb')) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "predicting-poverty-replication",
   "language": "python",
   "name": "predicting-poverty-replication"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
